<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="计算机视觉,深度图补全," />










<meta name="description" content="Learning Guided Convolutional Network for Depth CompletionAbstract（引导滤波：根据引导图动态生成内容相关且大小可变的引导卷积核） 密集深度感知对于自动驾驶和其他机器人应用至关重要。 但是，现代的LiDAR传感器只能提供稀疏的深度测量。因此，有必要补全稀疏的liDAr数据，其中通常使用同步引导的RGB图像来帮助深度数据的补全。许多神经">
<meta property="og:type" content="article">
<meta property="og:title" content="GuideDepthCompletion">
<meta property="og:url" content="http://yoursite.com/2020/04/07/GuideDepthCompletion/index.html">
<meta property="og:site_name" content="Blind Lover">
<meta property="og:description" content="Learning Guided Convolutional Network for Depth CompletionAbstract（引导滤波：根据引导图动态生成内容相关且大小可变的引导卷积核） 密集深度感知对于自动驾驶和其他机器人应用至关重要。 但是，现代的LiDAR传感器只能提供稀疏的深度测量。因此，有必要补全稀疏的liDAr数据，其中通常使用同步引导的RGB图像来帮助深度数据的补全。许多神经">
<meta property="og:image" content="http://yoursite.com/2020/04/07/GuideDepthCompletion/GuideConvolutionModule.png">
<meta property="og:image" content="http://yoursite.com/2020/04/07/GuideDepthCompletion/GuideDepthCompletion/GuideConvolutionModule.png">
<meta property="og:image" content="http://yoursite.com/2020/04/07/GuideDepthCompletion/NetworkArchitectural.png">
<meta property="og:image" content="http://yoursite.com/2020/04/07/GuideDepthCompletion/GuideDepthCompletion/NetworkArchitectural.png">
<meta property="og:image" content="http://yoursite.com/2020/04/07/GuideDepthCompletion/Tabel1.png">
<meta property="og:image" content="http://yoursite.com/2020/04/07/GuideDepthCompletion/Figure3.png">
<meta property="og:image" content="http://yoursite.com/2020/04/07/GuideDepthCompletion/Tabel2.png">
<meta property="og:image" content="http://yoursite.com/2020/04/07/GuideDepthCompletion/Figure5.png">
<meta property="og:image" content="http://yoursite.com/2020/04/07/GuideDepthCompletion/Figure6&Figure7.png">
<meta property="og:image" content="http://yoursite.com/2020/04/07/GuideDepthCompletion/Figure8.png">
<meta property="og:image" content="http://yoursite.com/2020/04/07/GuideDepthCompletion/Tabel4&Tabel5.png">
<meta property="article:published_time" content="2020-04-07T13:42:11.000Z">
<meta property="article:modified_time" content="2020-04-07T13:50:25.949Z">
<meta property="article:author" content="kaixiang Liu">
<meta property="article:tag" content="计算机视觉">
<meta property="article:tag" content="深度图补全">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://yoursite.com/2020/04/07/GuideDepthCompletion/GuideConvolutionModule.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2020/04/07/GuideDepthCompletion/"/>





  <title>GuideDepthCompletion | Blind Lover</title>
  








<meta name="generator" content="Hexo 4.2.0"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Blind Lover</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/04/07/GuideDepthCompletion/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="kaixiang Liu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/zhenzhu.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Blind Lover">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">GuideDepthCompletion</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-04-07T21:42:11+08:00">
                2020-04-07
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%B2%BE%E8%AF%BB%E8%AE%BA%E6%96%87/" itemprop="url" rel="index">
                    <span itemprop="name">精读论文</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2020/04/07/GuideDepthCompletion/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2020/04/07/GuideDepthCompletion/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
             <span id="/2020/04/07/GuideDepthCompletion/" class="leancloud_visitors" data-flag-title="GuideDepthCompletion">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="Learning-Guided-Convolutional-Network-for-Depth-Completion"><a href="#Learning-Guided-Convolutional-Network-for-Depth-Completion" class="headerlink" title="Learning Guided Convolutional Network for Depth Completion"></a>Learning Guided Convolutional Network for Depth Completion</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>（引导滤波：根据引导图动态生成内容相关且大小可变的引导卷积核）</p>
<p>密集深度感知对于自动驾驶和其他机器人应用至关重要。 但是，现代的LiDAR传感器只能提供稀疏的深度测量。因此，有必要补全稀疏的liDAr数据，其中通常使用同步引导的RGB图像来帮助深度数据的补全。许多神经网络已经应用在此任务中。 但是，它们通常通过特征concatnation或逐元素相加来简单的融合激光雷达数据和rGB图像信息。 受到引导图像滤波的启发，我们设计了一种新颖的引导网络，以根据引导图像预测kernel的权重。然后将这些预测的kernel应用于提取深度图像特征。 通过这种方式，我们的网络生成了content-dependent并且spatially-yariant的kernel，用于多种特征融合。 动态生成的kernel变量可能导致GPU内存消耗过大和计算开销。我们进一步设计了卷积分解以减少计算和内存消耗。 GPU内存的减少使特征融合可以在多阶段方案中工作。 我们进行了全面的实验，以验证我们在真实的室外，室内和合成数据集上的方法。 我们的方法效果很好。它优于NYUv2数据集上的最新方法，并在提交时在KITTI数据集深度补全任务中排名第一。 它还在不同的3D点密度，各种光照和天气条件下以及跨数据集评估中提供了强大的泛化能力。 该代码将开源。</p>
<a id="more"></a>
<h2 id="Ⅰ-INTRODUCTION"><a href="#Ⅰ-INTRODUCTION" class="headerlink" title="Ⅰ. INTRODUCTION"></a>Ⅰ. INTRODUCTION</h2><p>稠密深度感知对于许多机器人应用（例如自动驾驶或其他移动机器人）至关重要。 对观察到的图像进行准确的密集深度感知是解决以下任务（如避开障碍物，检测或识别物体以及3D场景重建）的先决条件。虽然深度摄像头可轻松用于室内场景，但室外密集的深度感知主要依靠立体视觉或LiDAR传感器。 立体视觉算法<a href="#1">[1]-[4]</a> 在薄且不连续的物体重建中仍然有很多困难。到目前为止，激光雷达传感器提供了最可靠，最准确的深度感应，并已广泛集成到许多机器人和自动驾驶汽车中。 然而，当前的LiDAR传感器仅获得稀疏的深度测量值，例如。 垂直方向上有64条扫描线对于像机器人导航这样的实际应用，这种稀疏的深度感应是不够的。因此，从稀疏的激光雷达输入估计密集深度图对于学术研究和工业应用都具有巨大的价值。许多关于该研究的最新文章<a href="#5">[5]-[7]</a>都采用深度学习作为方法，并利用附加的同步rGB图像进行深度补全。与传统方法<a href="#8">[8]-[10]</a>相比，这些方法有了明显的改进。例如<a href="#6">[6]</a>训练一个网络来从rgb图像和激光雷达数据估计表面法线，并进一步使用恢复的表面法线来指导深度补全。<a href="#7">[7]</a>利用相邻视频帧之间的光一致性来完成深度。<a href="#11">[11]</a>采用深度损失和语义损失进行监督。尽管这些工作提出了不同的方法，但在多模式特征融合中它们基本上共享相同的方案。 具体来说，这些工作采用诸如concatnation或逐元素相加的操作，将稀疏深度和rgb图像中的特征向量直接融合在一起，以进行进一步处理。但是，当考虑异构数据和复杂环境时，通常使用的concatnation或逐元素加法运算不太合适。通过这种简单的方法很难充分利用RGB图像作为引导的潜力。相反，我们建议使用更复杂的融合模块来改善深度补全任务的性能。</p>
<p>我们的工作灵感来自于引导滤波<a href="#13">[13],[14]</a> 。在引导滤波中，一个像素的输出是附近像素的加权平均值，其中权重是引导图像的函数。对于RGB和深度图像<a href="#15">[15],[17]</a>的补全/超分辨率，已经采用了这种策略。受到引导滤波的启发，我们寻求学习一种引导网络，以根据输入图像自动生成spatially-variant的卷积核，然后通过引导卷积模块将其应用到稀疏深度图像中提取特征。与引导滤波中手工生成kernel相比<a href="#13">[13]</a> ，我们的端到端学习型网络结构具有产生更强大kernel的潜力，这些kernel可以与场景上下文达成深度融合。与标准卷积模块相比，标准卷积模块的kernel在空间上是不变的，并且所有位置的像素都共享同一kernel，而我们的引导卷积模块具有在空间上可变的kernel，这些kernel会根据内容自动生成。 因此，我们的网络更强大，可以处理深度补全任务中的各种挑战性情况。</p>
<p>使用空间可变kernel的明显缺点是gpu内存消耗大，这也是卷积神经网络中参数共享的原因。 尤其是在多级融合中应用空间可变卷积模块进行深度补全时，计算平台甚至无法承受大量的gPu内存消耗（有关内存和计算的讨论，请参阅第III-C小节）。因此，寻找使网络可用的实用方法并非易事。 受最新网络压缩技术的启发<a href="#18">[18]</a>，我们将引导卷积模块中的卷积运算分解为两个阶段，一个是空间变化的逐通道卷积阶段，一个是空间不变的跨通道卷积阶段。 通过使用这种新颖的因式分解，我们极大地减少了GPU内存，从而可以在现代gpu设备中将引导卷积模块与强大的编码器-解码器网络多级集成。</p>
<p>我们在室外和室内数据集（来自真实场景和合成场景）上评估了所提出的方法。 在提交论文时，它的性能超过了KITTI深度补全benchmark的SOTA，并且排名第一。全面的ablation studies证明了我们方法中使用的每个组件和融合策略的有效性。与其他深度补全方法相比，我们的方法在室内NYUv2数据集上也达到了最佳性能。最后，我们的模型在不同深度点密度，各种光照和天气条件下以及跨数据集评估中都表现出强大的鲁棒性。 我们的代码将在<a href="https://github.com/kakaxi314/guidenet上发布。" target="_blank" rel="noopener">https://github.com/kakaxi314/guidenet上发布。</a></p>
<h2 id="Ⅱ-RELATED-WORK"><a href="#Ⅱ-RELATED-WORK" class="headerlink" title="Ⅱ. RELATED WORK"></a>Ⅱ. RELATED WORK</h2><p>根据是否有用于引导深度补全的RGB图像，以前的方法可以大致分为两类：仅深度方法和图像引导方法。 我们简要回顾了这些技术以及与我们的网络设计相关的其他文献。</p>
<p><strong>Depth-only Methods：</strong> 这些方法使用稀疏或低分辨率深度图像作为输入来生成全分辨率深度图。一些早期方法是基于压缩感觉理论<a href="#8">[8]</a>或组合的wavelet-contourlet dictionary<a href="#9">[9]</a>来重建密集视差图<a href="#8">[8]</a>或深度图<a href="#9">[9]</a>。<a href="#10">[10]</a>使用一系列手工制作的常规算子，例如扩张，闭孔，填孔和模糊等，将稀疏的深度图转换为密集的图。最近，基于深度学习的方法取得了很好的结果。<a href="#5">[5]</a>提出了一个稀疏不变cnn，通过使用observation mask来处理稀疏数据或特征。<a href="#19">[19]</a>通过生成full depth以及具有标准化卷积的置信度图来解决深度补全问题。<a href="#20">[20]</a>结合压缩感知和深度学习进行深度预测。这些方法的主要重点是设计适当的运算符，例如稀疏不变CNN<a href="#51">[51]</a>，处理稀疏输入并将这些备用信息传播到整个图像。</p>
<p>在深度图像超分辨率方面，某些方法利用成对的低分辨率和高分辨率深度图像块数据库<a href="#21">[21]</a>或自相似搜索<a href="#22">[22]</a>来生成高分辨率深度图像。 一些方法<a href="#23">[23],[24]</a>提出了通过字典学习解决深度图像超分辨率的方法。<a href="#25">[25]</a>使用深度网络生成高分辨率的深度图以及深度不连续性，并将它们输入到变分模型中以细化深度。与这些深度图像超分辨率方法不同，这些方法以密集且规则的深度图像作为输入。 相反，我们方法中的深度输入是稀疏且不规则的，而且我们无需任何进一步的优化或后处理即可端对端地训练模型。</p>
<p><strong>Image-guided Methods：</strong>这些方法通常会获得更好的效果，因为它们利用了附加的rgb图像，可以提供RGB图像的语义信息，边缘信息或表面信息等。早期的工作主要是通过双边滤波解决深度图像超分辨率问题<a href="#16">[16]</a> 或全局能量最小化<a href="#26">[26]-[28]</a>，其中由图像引导的深度补全有<a href="#16">[16]</a>，<a href="#26">[26]-[28]</a>，由语义分割引导的有<a href="#29">[29]</a>或边缘信息<a href="#30">[30]</a>引导的。</p>
<p>最近，<a href="#31">[31]</a>建议从深层网络预测表面法线和遮挡边界，并进一步利用它们来帮助在室内场景中补全深度。<a href="#6">[6]</a>将相似的表面法线作为引导扩展到室外环境，并从稀疏的LiDAR数据中恢复密集深度。<a href="#7">[7]</a>提出了一种自监督网络，以探索相邻视频帧之间的光一致性，以补全深度。<a href="#32">[32]</a>提出了三种稀疏不变操作来处理稀疏输入。 <a href="#33">[33]</a>将他们的置信度传播<a href="19">[19]</a>与RGB信息结合起来解决了这个问题。<a href="#34">[34]</a>使用两个并行网络来预测深度并学习不确定性以融合两个结果。<a href="#35">[35]</a>利用Cnn学习相邻像素之间的亲和度，以帮助深度估计。</p>
<p>尽管已经提出了各种方法来对参考RGB图像进行深度完成，但它们在融合深度和图像特征方面几乎共享相同的策略，这是简单的concretion或逐元素相加操作。 本文在引导滤波<a href="#13">[13]</a>的启发下，提出了一种新颖的用于特征融合的引导卷积模块，以更好地利用rgb图像中的引导信息。</p>
<p><strong>Joint Filtering and Guided Filtering：</strong>我们的方法也与联合双边滤波<a href="#14">[14]</a>和引导滤波<a href="#13">[13]</a>有关。 联合/引导滤波使用参考图像或引导图像作为先验，旨在将结构信息从参考图像转移到目标图像，以实现彩色/深度图像超分辨率<a href="#15">[15]-[16]</a>，图像恢复<a href="#36">[36]</a>等任务。</p>
<p>早期的联合滤波方法<a href="#37">[37]-[39]</a>探讨了目标图像和参考图像之间的共同结构，并将其表示为迭代能量最小化的问题。<a href="#40">[40]</a>提出了一种基于CNN的联合滤波，用于图像降噪，深度上采样等，但是联合滤波被实现为简单的特征concatenation。<a href="#41">[41]</a>通过深度网络生成仿射参数，进行颜色变换，以增强图像。<a href="#42">[42]</a>采用与<a href="#41">[41]</a>类似的双边学习方案，但是生成双边权重，并将它们一次应用于预先获得的深度图以进行深度细化。 相反，我们的引导卷积模块处理图像特征，并在编码器-解码器网络的多个阶段中充当可灵活插入的组件。在<a href="#43">[43]</a>中，提出一个有引导的滤波层来执行联合上采样，这与我们的工作很接近。 它直接重新构造了常规的引导滤波器<a href="#13">[13]</a>，使其可区分为神经网络层。所以kernel的权重由相同的引导滤波器<a href="#13">[13]</a>的 闭合形式方程生成，以对输入图像进行滤波。正如引导滤波器的作者在会议论文中所评论的那样，这种运算符不适用于填充稀疏激光雷达点<a href="#44">[44]</a>。我们的方法也受到引导滤波器的启发<a href="#13">[13]</a>。 我们考虑从引导图像中学习更多通用且功能强大的kernel，并将这些kernel应用于多模态特征的融合以完成深度补全任务，而不是从特定的闭合形式方程生成引导滤波器kernel。</p>
<p><strong>Dynamic Filtering：</strong>另一方面，在卷积神经网络中，动态滤波网络（DFN）<a href="#45">[45]</a>是一类广泛的方法，其中，网络根据输入图像动态生成滤波器kernel，以对输入特征进行局部空间变换等操作。 在<a href="#45">[45]</a>中首先提出的一般概念主要是在视频（和立体声）预测上进行评估，并以先前的帧作为输入。最近，已经开发了基于DFN的一些应用和扩展。 可变形卷积<a href="#46">[46]</a>可以动态生成固定几何结构的偏移量，通过关注采样位置可以将其视为DFN的扩展。<a href="#47">[47]</a>将DFN扩展到空间域的图形信号中，在其中为每个特定的输入样本动态生成滤波器权重，并以边缘标签为先验条件。<a href="#48">[48]</a>提出了通过使用多个采样邻域来动态生成具有较大感受野权重的DFN的扩展。</p>
<p>我们的kernel生成方法与DFN有着相同的理念，可以被视为侧重于多模态数据，多阶段特征融合的变体和扩展。<a href="#45">[45]</a>生成的空间可变kernel消耗大量GPU内存，因此仅可以在低分辨率图像或功能上使用一次。 但是，多级特征融合对于深度完成任务中稀疏深度和彩色图像的特征提取至关重要，但以前的DFN论文尚未对此进行研究。 为了解决这个问题，我们设计了一种具有卷积因子分解的新型网络结构，并进一步讨论了融合策略对深度完成结果的影响。</p>
<h2 id="Ⅲ-THE-PROPOSED-METHOD"><a href="#Ⅲ-THE-PROPOSED-METHOD" class="headerlink" title="Ⅲ.  THE PROPOSED METHOD"></a>Ⅲ.  THE PROPOSED METHOD</h2><p>给定将LiDAR点投影到具有校准参数的并且以RGB引导图像$I$为参考的图像平面而生成的稀疏深度图$S$，深度补全的目的是生成整个图像的密集深度图$D$。RGB图像可以提供非常有用的深度补全任务的信息，因为它描述了对象边界和场景内容。</p>
<p>为了解释我们在$I$的引导下将$S$升级到$D$的引导卷积网络，我们首先简要回顾一下引导滤波，这Ⅲ-A小节中的引导卷积模块的由来。然后，我们在Ⅲ-B小节中详细介绍了引导卷积模块的设计，并在III-C小节中介绍了一种新颖的卷积因式分解。接下来，我们将说明如何在通用的编解码器网络和多级中使用该模块以及在III-D小节中我们的方法中使用的融合方案。 最后，我们在III-E小节中提供了实现细节，包括超参数设置。</p>
<h3 id="A-Guided-Image-Filtering"><a href="#A-Guided-Image-Filtering" class="headerlink" title="A. Guided Image Filtering"></a>A. Guided Image Filtering</h3><p>引导滤波<a href="#13">[13]</a>根据引导图像生成空间变量滤波器。 在我们设定的深度完成任务中，此方法会将$D$中像素$i$的值计算为$S$中附近像素的加权平均值，即<br>$$D_i=\sum_{j\in N(i)}W_{ij}(I)S_j \tag{1}$$<br>在此，$i$，$j$是像素索引，$M(i)$是像素i的局部邻域。 根据引导图像$I$和类似于<a href="#49">[49]</a>中的Matting拉普拉斯算子的人工设计的闭式方程，计算出内核权重$W_{ij}$。除非特别说明，否则我们将省略图像或特征通道的索引以简化表示法。</p>
<p>这种引导的图像滤波可能会应用于图像超分辨率，如<a href="#17">[17]</a>。 但是，我们输入的LiDAR点稀疏且不规则。 正如<a href="#44">[44]</a>的作者所指出的，在稀疏输入上，引导滤波不能很好地工作。 这促使我们从引导图像$I$中学习更多通用且功能强大的滤波器kernel，而不是使用手工生成的函数来生成kernel。然后，我们将kernel应用于融合多模式特征，而不是直接输入图像滤波。</p>
<h3 id="B-Guided-Convolution-Module"><a href="#B-Guided-Convolution-Module" class="headerlink" title="B. Guided Convolution Module"></a>B. Guided Convolution Module</h3><p>在这里，我们详细介绍了引导卷积模块的设计，该模块会生成内容相关的和空间变化的kernel以补全深度。</p>
<p>如图1所示，我们的引导式卷积模块将用作可灵活插入的组件，以多级融合来自RGB和深度图像的功能。它会根据引导图像的特征$\mathcal I$自动生成卷积核，并将其应用于稀疏深度图特征$\mathcal S$。这里，$\mathcal I$和$\mathcal S$分别是从引导图像$I$和稀疏深度图$S$中提取的特征。我们将此引导卷积模块的输出表示为$D$，这是深度图像的特征。<br>$$D=W^G(\mathcal I;\Theta)\otimes \mathcal S \tag{2}$$<br>其中$W^G$是我们的网络根据输入的引导图像特征$\mathcal I$生成的kernel，并且进一步取决于网络参数$\Theta$。在这里，$\otimes$表示卷积运算。</p>
<p>图2（a）说明了我们可学习的引导卷积模块的设计。 有一个“kernel生成层（KGL）”，可以根据图像特征$\mathcal I$生成kernel即$W^G$。KGL的参数为$\Theta$。 原则上，我们可以对此KGL采用任何可区分的操作。 由于我们处理的是网格图像，因此卷积层是完成此任务的首选。因此，最简单的实现是直接应用卷积层以在深度特征图上生成卷积运算所需的所有kernel权重。需要注意的是，$W^G$依赖于内容并且在空间上是可变的。内容相关意味着根据图像内容$\mathcal I$动态生成引导核$W^G$。空间变化意味着将不同的核应用于稀疏深度特征$\mathcal S$的不同空间位置。相比之下，$\Theta$一旦学习到，其在空间上固定并且跨不同的输入图像。</p>
<p>内容依赖型和空间可变型kernel的优点是双重的。首先，这种kernel允许网络将不同的滤波器应用于不同的对象（以及不同的图像区域）。之所以有用，是因为例如汽车上的深度分布可能不同于道路上的深度分布（附近和远处的汽车也具有不同的深度分布）。因此，根据图像内容和空间位置动态生成核将会由这些不同的特征的信息组成。其次，在训练期间，空间不变的kernel的梯度是来自下一层的所有图像像素的平均值。这样的平均值更可能导致梯度接近于零，甚至可以认为所学习的kernel对于每个位置都远非最佳，这可能会产生<a href="#48">[48]</a>所示的次优结果。相比之下，空间可变的kernel可以缓解此问题，并使训练表现更好，从而获得更好的结果。</p>
<p><strong>C. Convolution Factorization：</strong> 但是，简单的地生成和应用这些空间可变的kernel将消耗大量的GPU内存和计算资源。将引导卷积模块集成到编码器-解码器网络的多级融合中，对于现在的GPU设备而言，巨大的GPU内存消耗是无法承受的。为应对这一挑战，受到了最新网络压缩技术的启发（例如 MobileNets<a href="#18">[18]</a>），我们设计了一种新颖的分解和匹配的网络结构，将引导卷积模块分为两个阶段，以提高存储和计算效率。 此步骤对于使网络切实可行至关重要。</p>
<p>如图2（b）所示，第一阶段是一个逐通道卷积层，其中深度特征$S_m$的第$m$个通道与生成的滤波器的核${W’}_m^G$的相应通道卷积。 这些卷积在空间上仍然是变化的。 第一阶段之后的输出深度特征$D’$变为：<br>$${D’}_m={W’}_m^G(\mathcal I;\Theta’)\otimes{\mathcal S}_m \tag{3}$$<br>其中$\Theta’$和${W’}^G$分别是第一阶段的KGL参数和引导kernel。在此阶段，KGL由标准卷积层实现。</p>
<p>第二阶段是跨通道卷积层，其中1 x 1卷积会聚合跨不同通道的特征。 这个阶段仍然取决于内容，但是在空间上是不变的。核权重也从引导图像特征$\mathcal I$生成，但是在所有像素之间共享。具体来说，我们首先在每个通道上分别对引导图像特征$\mathcal I$进行平均池化，以获得大小为$M\times1\times1$的中间图像特征$\mathcal I’$，其中$M$是$\mathcal I$的通道数。然后，我们将$\mathcal I’$馈入全连接层以生成大小为$M\times N\times1\times1$的引导核${W’’}^G$，其中$N$是密集深度特征$D$的通道数。最后，将${W’’}^G$应用于从逐通道卷积层获得的深度特征$D’$以获得最终的深度特征$D$。该过程数学表达如下：<br>$$D={W’’}^G(\mathcal I’;\Theta’’)\otimes D’ \tag{4}$$<br>其中$\Theta’’$是全连接层中的参数。在等式（4）中，${W’’}^G$在空间上是不变的，并且由所有像素共享。应用于$D’$的卷积为$1 \times 1$卷积，在$N$通道的深度特征$D$中聚合$M$通道的特征。</p>
<img src="/2020/04/07/GuideDepthCompletion/GuideConvolutionModule.png" class="">
<!-- <img src="./GuideDepthCompletion/GuideConvolutionModule.png" /> -->

<p><strong>D. Network Architecture：</strong> 图1说明了提出的网络的总体结构。 它基于两个可跳跃的的编码器-解码器网络。 在这里，我们将分别取RGB图像$I$和稀疏LiDar深度图像$S$的两个网络称为GuideNet和DepthNet。GuidedNet旨在从RGB图像中学习具有低级和高级信息的分层特征表示。 此类图像特征用于自动生成空间可变和内容相关的kernel，以进行深度特征提取。DepthNet将LiDAR深度图像作为输入，并在编码器阶段由引导卷积模块逐步融合分层图像特征。 然后，它在解码器阶段对密集深度图像进行回归。GuidedNet和DepthNet的两种编码器都包含一堆resNet块<a href="#12">[12]</a>。具有stride的卷积层用于在编码器阶段将特征聚合为低分辨率，而在解码器阶段反卷积层将特征图上采样为高分辨率，我们还在GuideNet和DepthNet的开始以及depthNet的末尾添加了标准卷积层。</p>
<p>请注意，在特征融合过程中，我们采用了一种新颖的融合方案，将guidedNet的解码器特征融合到编码器特征中，而不是现有方法中广泛使用的早期或晚期融合方案<a href="#6">[6]</a>，<a href="#7">[7]</a>，<a href="#34">[34]</a>。 在我们的网络中，图像特征可作为生成深度特征表示的引导。因此，与编码器功能相比，GuideNet中解码器阶段的功能更可取，因为它们拥有更高级的上下文信息。 另外，相比之下只能融合一次，我们在多阶段融合了两中源信息，显示出更强大，更可靠的结果。在IV-D小节中可以找到更多的比较和分析。</p>
<img src="/2020/04/07/GuideDepthCompletion/NetworkArchitectural.png" class="">
<!-- <img src="./GuideDepthCompletion/NetworkArchitectural.png" /> -->

<p><strong>E. Implementation Details：</strong></p>
<p>(1) Loss Function:在训练过程中，我们采用均方误差（MSE）来计算ground truth和预测深度之间的损失。对于实际数据，ground truth通常是半稠密的，因为很难为每个像素收集地面真实深度。 因此，在计算训练损耗时，我们仅考虑参考ground truth深度图中的有效像素。 最终的损失函数是：<br>$$L=\sum_{p\in P_v}\left|D_p^{gt}-D_p\right|^2 \tag{9}$$<br>其中$P_v$代表有效像素的集合，$D_p^{gt}$和$D_p$分别代表每个像素的真值和预测值。</p>
<p>(2) Training Setting:我们使用Adam作为优化器，初始学习率为$10^{-3}$，权重衰减为$10^{-6}$，学习率每50k次迭代衰减一半。我们使用两块GTX 1080Ti进行训练，batch size为8。网络训练阶段使用了同步的跨GPU批量归一化<a href="#51">[51,52]</a>。我们的方法是端到端的从头训练，某些最新的方法使用了额外的数据集进行训练，例如DeepLidar使用综合数据集训练得到场景的表面法线。而<a href="#34">[34]</a>的作者使用 Cityscapes的预训练模型作为网络初始化。</p>
<h2 id="IV-EXPERIMENTS"><a href="#IV-EXPERIMENTS" class="headerlink" title="IV. EXPERIMENTS"></a>IV. EXPERIMENTS</h2><p>我们进行了全面的实验，以在室外和室内数据集（在真实世界和合成场景中捕获）上验证我们的方法。 我们首先在IV-A和IV-B部分分别介绍实验中使用的所有数据集和评估指标。 然后，由于自动驾驶是深度完成的主要应用，我们在IV-C小节的户外场景VKitti数据集上将我们的方法与最新方法进行了比较。 接下来，对IV-D小节中的kitti验证集进行了广泛的消融研究，以研究我们的方法中使用的每个网络组件和融合方案的影响。 在IV-E小节中。我们在室内场景NYUv2数据集上验证了所提出方法的性能。 最后，在第IV-F小节中，我们在各种设置下进行实验，包括不同密度的输入深度，在各种光照和天气条件下捕获的RGB图像以及跨数据集评估，以证明我们方法的泛化能力。</p>
<h3 id="A-dataset"><a href="#A-dataset" class="headerlink" title="A. dataset"></a>A. dataset</h3><p>KITTI深度补全数据集包含用于训练的86898帧，用于验证的1000帧和用于测试的另外1000帧。它为排名提交提供了公共排行榜。ground truth是通过LiDAR扫描生成的。这些扫描的点将进一步通过立体图像对进行验证，以消除噪点。由于图像顶部很少出现LiDAR点，因此与<a href="#34">[34]</a>一样，输入图像被裁剪为256 x 1216，用于训练和测试。</p>
<p>Virtual KITTI Dataset：虚拟KITTI数据集<a href="#53">[53]</a>是一个合成数据集，其中的虚拟场景是从真实世界的KITTI视频序列中克隆的。除了从KITTI序列中克隆的5个虚拟图像序列外，它还在各种光照条件（如早晨，日落）和天气条件（如雾，雨）下生成相应的图像序列，共17,000个图像帧。为了生成稀疏的LiDaR点，我们使用KITTI数据集中相应图像帧的稀疏深度作为遮罩，以从稠密的地面真实深度获取稀疏样本，而不是从稠密深度图上进行随机采样，从而使稀疏深度的分布图像接近实际情况。我们将整个虚拟KITTI数据集拆分为训练和测试集，以分别微调和评估我们的模型。由于目的是验证模型在各种光照和天气条件下的鲁棒性，因此我们仅在天气良好的原始“克隆”条件下对模型进行微调，使用000l 、、 0002、0006和0018的序列进行训练，具有各种天气和光照条件的0020序列用于评估。总之，我们有1289帧用于微调，每种条件有837帧用于评估。</p>
<p>NYUv2 Dataset：NYUv2数据集<a href="#57">[57]</a>由微软kinect在464个室内场景中捕获的RGB图像和深度图像组成。 按照先前的深度补全方法[6]，[35]，[54]的类似设置，我们的方法在从训练集中统一采样的50k图像上进行训练，并在654个带有官方标签的测试集上进行测试，作为评估的预处理， 深度值使用官方工具箱进行绘制，该工具箱采用着色方案[58]来填充缺失值。 对于训练和测试集。 原始尺寸为640 X 480的帧通过双线性插值进行一半下采样，然后中心裁剪为304 x228。稀疏的输入深度是通过从密集的ground truth中进行随机采样而生成的，因为我们网络的输入分辨率必须为 作为32的倍数，我们将图像进一步填充到320x 256作为我们方法的输入，但仅评估大小为304x 228的有效区域，以与其他方法保持公平的比较。</p>
<p>SUN RGBD Dataset ：SUN RGBD数据集[59]是一个室内数据集，其中包含来自许多其他数据集[57]，[60]，[61]的RGB-D图像。 我们仅将SUN RGBD数据集用于跨数据集评估。 由于NYUv2数据集是SUn RGBD数据集的子集。 为了避免重复，我们将它们排除在评估之外。 我们将在不同场景下捕获的所有图像分辨率都调整为640 X 480，总共评估了3944个图像帧的模型，其中Kinect VI捕获了555帧，Asus Xtion摄像机捕获了3389。 NYUv2数据集的相同预处理方法用于填充深度图。 请注意，华硕Tion相机拍摄的帧更具挑战性，因为数据来自其他设备。</p>
<h3 id="B-Evaluation-Metrics"><a href="#B-Evaluation-Metrics" class="headerlink" title="B. Evaluation Metrics"></a>B. Evaluation Metrics</h3><p>遵循KITTI基准和现有深度补全方法[6]，[7、35]，对于室外场景，我们使用以下四个标准度量进行评估：均方根误差（RMSE），均值绝对误差（MAE），均方根反深度的误差（iRMSE）和反深度的平均绝对误差（iMAE）。其中，RMSE和MAE直接测量深度精度，而RMSE则更敏感，因此被选为在KITTI排行榜上排名提交的主要指标。 iRMSE和iMae会计算反深度的平均误差，这可以减轻遥远点的权重。</p>
<h3 id="C-Experiments-on-KITTI-Dataset"><a href="#C-Experiments-on-KITTI-Dataset" class="headerlink" title="C. Experiments on KITTI Dataset"></a>C. Experiments on KITTI Dataset</h3><p>我们首先在KITTI深度补全数据集上评估我们的方法。 我们的方法是从头开始对训练集进行端到端训练，并将性能与测试集上的最新方法进行比较。 表一列出了我们的方法与在KITTI排行榜上排名第一的其他方法的定量比较。在提交论文时，该方法在主要RMSE指标下排名第一，并超过所有其他方法，并且在其他评估指标上也具有可比的性能。</p>
<img src="/2020/04/07/GuideDepthCompletion/Tabel1.png" class="">
<!-- ![](./GuideDepthCompletion/Tabel1.png) -->
<p>图3显示了KITTI测试集上几种SOTA的比较结果。我们的结果显示在最后一行。 虽然所有方法通常都能提供合理的结果，但我们估算的深度图揭示了更多细节，并且在对象边界附近更准确。 例如，我们的方法可以更好地恢复人的两臂之间的背景深度，如图3中的红色圆圈所示。我们的方法的预测深度在黑色汽车区域拥有最准确的轮廓。</p>
<img src="/2020/04/07/GuideDepthCompletion/Figure3.png" class="">
<!-- ![](./GuideDepthCompletion/Figure3.png) -->
<p>此外，为了验证引导卷积模块是否真的学习了内容相关和空间变化的信息以利于深度补全。在图4中的最早期融合阶段我们选取了引导核$W’^G$的一个通道进行可视化。这是通过应用Prewitt操作<a href="#62">[62]</a>在每个$K \times K$的卷积核上分别得到x和y方向的权重偏移量总和实现的。然后，我们在每个像素处获得一个2D向量，并通过颜色代码对其进行可视化，就像可视化光流的方式一样。我们可以容易地看到，具有相似梯度方向的边界或具有相似法线方向的表面共享相似的颜色代码。请注意，由于在深度神经网络中难以解释内核权重，因此用于可视化引导内核的方法非常困难。而且，网络仅由半密集深度进行监督。几乎没有可能在没有直接语义监督的情况下使每个对象在可视化中拥有自己的颜色代码，因为语义信息是人为定义的，与深度监督几乎没有关系。在某种程度上。这种可视化确认了引导的内核与图像内容是一致的。因此，引导核可能有助于深度补全。</p>
<h3 id="D-Ablation-Studies"><a href="#D-Ablation-Studies" class="headerlink" title="D. Ablation Studies"></a>D. Ablation Studies</h3><p>为了调查每个网络组件和融合方案对最终性能的影响，我们对KITTI验证数据集进行了消融研究。 具体来说，我们评估了我们网络的几种不同变体。 定量比较总结于表II。</p>
<p>(1) Comparison with Feature Addition/Concatenation:<br>现有方法通常使用加法或串联进行多模态特征融合。 为了与它们进行比较，我们通过添加或串联功能来替换网络中的所有引导卷积模块，但保持其他网络组件和设置不变。 结果分别表示为“ Add。”和“ Concat。”。与我们的引导卷积模块相比，简单的特征添加或串联显着恶化了结果，均方根值分别增加了31. 59 mm和24.35 mm。</p>
<p>我们可以看到ADD比Concat差一点。这也是合理的，因为图像和深度特征是来自不同来源的异构数据。通过应用加法，我们以相同的方式隐式对待这两个不同的功能，这会导致性能下降。 实际上，大多数最先进的方法[61]，[71],[33]都采用Concat的方法融合了不同深度和图像。</p>
<img src="/2020/04/07/GuideDepthCompletion/Tabel2.png" class="">
<!-- ![](./GuideDepthCompletion/Tabel2.png) -->

<p>(2) FusionSchemeofGuideNetandDepthNet:如第III-D小节所述，我们的方法不是像现有方法[6]，[71]，[34]那样使用早期或晚期特征融合，而是将引导网络的解码器特征与深度网络的编码器特征融合在一起。 为了验证这种融合方案的有效性，我们训练并评估了将引导网络的解码器特征融合到深度网络的解码器特征（称为“ DD融合”），以及将引导网的编码器特征融合到深度网络的编码器特征的性能。 在后者中，由于不再使用引导网络的解码器结构，所以将其移除。这样，我们的方法可以看作是D-E Fusion。</p>
<p>表II比较了我们方法的“ E-E融合”和“ D-D融合”结果。 E-E融合的性能下降验证了我们之前的分析，即解码器图像特征拥有更多的高级上下文信息，因此可以更好地指导深度特征提取。D-D融合在解码器阶段将图像和深度功能融合在一起，会遭受更大的性能下降。比较D-D Fusion和我们的最终模型，我们得出结论，在深度特征提取的编码器阶段，图像引导更为有效。 这也是合理且易于理解的，因为早期提取的特征会影响后续特征的提取，特别是对于稀疏深度图像。</p>
<p>另一方面，即使是“ E-E融合”中较弱的融合策略也优于传统的特征相加或串联。 这归功于我们的引导式卷积模块，该模块可以生成内容相关的空间变量核，以促进深度完成。 该观察结果进一步证明了所提出的引导卷积模块的有效性。</p>
<p>(3) Fusion Scheme of Multi-stage Guidance:我们还设计了两个其他变体，以验证多阶段引导方案的有效性。为了进行比较，基于我们的引导网络，我们将除第一融合阶段中的模块以外的所有引导模块替换为concat，并将其称为第一引导。 从相同的角度来看，我们使用“最后一个引导”来指代仅保留最后一个融合阶段中的引导模块。将concat用于其他阶段的特征融合的结果是，concat可以比加法运算更好，如表Ⅱ所示。</p>
<p>我们可以看到，“第一引导和最后引导”的结果都比我们的多阶段指导方案差。 这证明了我们多阶段引导设计的有效性。 另外，“第一引导”的效果要比“最后引导”的要好一些。它还与我们的早期分析相结合，因为在早期提取的特征会影响后续的特征提取，因此图像引导在早期会更有效。 此外。 “第一引导”和“最后引导”的结果都比Concat更好。 它再次验证了设计的引导卷积模块是用于深度补全的强大功能融合方案。</p>
<h3 id="E-Experiments-on-NYUv2-Dataset"><a href="#E-Experiments-on-NYUv2-Dataset" class="headerlink" title="E. Experiments on NYUv2 Dataset"></a>E. Experiments on NYUv2 Dataset</h3><p>为了验证我们的方法在室内场景下的性能，我们直接在NYUv2数据集<a href="#57">[57]</a>上训练和评估了我们的引导网络，而没有进行任何特定的修改。</p>
<p>按照现有方法，我们分别使用200和500个稀疏LiDAR样本的设置来训练和评估我们的方法。与其他方法的定量比较示于表III，Bilateral<a href="#57">[57]</a>和CSPN[35]的结果来自CSPN[35]。 TGV的结果[28]，张等人[31]和DeepLiDAR[6]从DeepLiDAR[6]获得。通过使用公布的实现，我们得到了Ma等人的结果[54]。其有500个样本，NConv-CNN[33]有200个样本。 从结果中我们可以看到，在500个样本和200个样本的设置中，我们的方法均优于其他所有方法，而无需进行特殊修改，我们的方法在这5个评估指标中均排名第一。</p>
<p>我们还在图5中对测试集进行了定性比较。我们的方法与NConV-CNN[33]和Ma等人[54]进行了比较。[54]设置200个样本和500个样本。对比最明显的区域用青色框画出，以便于比较。从预测的深度，我们可以看到马等人的结果使整个图像平滑，并使小物体模糊。 即使“ NConv-CNN显示了很多清晰的深度预测，它在对象结构（尤其是薄的对象边界）上也会遭受明显的细节损失。我们的方法显示出与局部细节对齐的尖锐过渡并产生最佳结果。</p>
<img src="/2020/04/07/GuideDepthCompletion/Figure5.png" class="">
<!-- ![](./GuideDepthCompletion/Figure5.png) -->



<h3 id="F-Generalization-Capability"><a href="#F-Generalization-Capability" class="headerlink" title="F. Generalization Capability"></a>F. Generalization Capability</h3><p>为了证明我们方法的泛化能力，我们在不同点密度，各种光照和天气条件下以及跨数据集评估下测试其性能。</p>
<p>(1) Different Point Densities:我们在不同的点密度下测试我们方法的性能。 我们的模型是只在KITTI训练集上从头开始训练的模型，没有任何微调，以忠实地反映其泛化能力。为了进行比较，我们还使用它们的开源代码和作者训练的最佳性能模型，评估了另外两种最先进的方法，即NConv-CNN[33]和Sparse-To-Dense[7]。</p>
<p>首先，我们在KITTI验证集上以5种不同的密度级别改变激光雷达的输入。使用64线Velodyne LiDAR捕获KITTI数据集。 但是，考虑到高昂的传感器成本，实际的工业应用只能采用32线甚至16线激光雷达。为了分析稀疏度水平对最终结果的影响，我们在KITTI验证数据集中使用5种不同的激光雷达密度水平进行了测试，其中根据给定的比率对输入的LiDAR点进行了随机采样。 具体来说，我们的评估采用的密度比为0. 2，0.4，0.6，0.8和1.0。</p>
<p>图6显示了在各种LiDAR点密度下，我们的网络和NConv-CNN[33]和Sparse-To-Dense[7]的RMSE。随着密度的降低，NConv-CNN[33]表现出明显的性能下降，并且RMSE迅速增加。相比之下，我们的方法和Sparse-To-Dense[7]逐渐退化，并且始终优于NConv-CNN[33]。结果表明，在各种激光雷达点密度比下，我们的方法具有很强的泛化能力。</p>
<p>(2) Various Lighting and Weather Conditions: 我们分别在“雾”，“早晨”，“阴天”，“雨天”和“日落”条件下评估我们的方法和两个变体。图7描绘了三种方法在各种条件下的结果。 我们可以很容易地发现，与Add和Concat相比，我们的方法在所有条件下均能达到最佳均方根值。此外，我们的均方根值结果在所有情况下均保持稳定，这可以验证我们的方法在各种光照条件和天气情况下的泛化能力。 </p>
<img src="/2020/04/07/GuideDepthCompletion/Figure6&Figure7.png" class="">
<!-- ![](./GuideDepthCompletion/Figure6&Figure7.png) -->

<p>(3) Cross-dataset Evaluation: 为了展示我们方法的推广。 我们还使用在NYUv2数据集上训练的模型直接在SUN RGBD数据集上进行测试，从而进行跨数据集评估[59]。</p>
<p>表四和表五分别列出了Kinect Ⅵ和Asus Xtion相机捕获的数据集的比较结果。通过使用在NYUv2数据集上训练的比较模型来评估500个样本和200个样本。我们可以看到我们的方法仍然具有优于其他方法的RMSE，并且使用NYUv2数据集得到了接近KITTI的结果。结果证明了我们的方法强大的跨数据集泛化能力。 我们还在图8中给出了一些定量结果。在红色矩形中选择的前三行是Kinect VI捕获的图像的结果，而在绿色矩形中的最后三行是Xtion的结果。我们的方法的优越性可以从预测的深度轻松看出，尤其是所选区域。</p>
<img src="/2020/04/07/GuideDepthCompletion/Figure8.png" class="">
<img src="/2020/04/07/GuideDepthCompletion/Tabel4&Tabel5.png" class="">
<!-- ![](GuideDepthCompletion/Figure8.png)
![](GuideDepthCompletion/Tabel4&Tabel5.png) -->
<p>通过比较表III，表IV和表V中的结果，我们可以发现，这三种方法在Xtion收集的数据集上产生的结果都稍差一些，这可能是由不同相机的图像传感器和深度传感器的固有参数和外部参数之间的引起的 如何设计在不同设备之间具有更好泛化能力的方法是未来研究的有趣方向。</p>
<h2 id="V-CONCLUSION"><a href="#V-CONCLUSION" class="headerlink" title="V. CONCLUSION"></a>V. CONCLUSION</h2><p>我们提出一种引导式卷积网络，以RGB图像为引导，从稀疏和不规则LiDar点恢复密集深度。我们新颖的引导网络可以根据引导图像动态预测内容相关和空间变化的内核权重，以利于深度补全。我们进一步设计了卷积分解以减少GPU内存消耗，从而使我们的引导卷积模块可以通过多级融合方案应用于功能强大的编码器解码器网络。大量的实验和消融研究证明了我们的引导卷积网络的优越性能以及特征融合策略在深度补全方面的有效性。我们的方法不仅在室内和室外场景中都表现出了出色的效果，而且在不同点密度下都具有强大的泛化能力，各种光照和天气条件以及跨数据集评估。尽管本文专门针对深度补全问题，但我们认为将多源作为输入的计算机视觉中的其他任务也可以从我们的引导卷积模块的设计和方法中的融合方案中受益。</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/" rel="tag"># 计算机视觉</a>
          
            <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%9B%BE%E8%A1%A5%E5%85%A8/" rel="tag"># 深度图补全</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2020/04/06/MSG-CHN/" rel="next" title="MSG-CHN">
                <i class="fa fa-chevron-left"></i> MSG-CHN
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
    </div>
  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/zhenzhu.png"
                alt="kaixiang Liu" />
            
              <p class="site-author-name" itemprop="name">kaixiang Liu</p>
              <p class="site-description motion-element" itemprop="description">知心大姐姐的臭弟弟的blog</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives">
              
                  <span class="site-state-item-count">7</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">2</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">6</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/kaixiang-git" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://weibo.com/BlindLover0403" target="_blank" title="微博">
                      
                        <i class="fa fa-fw fa-globe"></i>微博</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://www.zhihu.com/people/liu-kai-40-21/activities" target="_blank" title="知乎">
                      
                        <i class="fa fa-fw fa-globe"></i>知乎</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Learning-Guided-Convolutional-Network-for-Depth-Completion"><span class="nav-number">1.</span> <span class="nav-text">Learning Guided Convolutional Network for Depth Completion</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract"><span class="nav-number">1.1.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ⅰ-INTRODUCTION"><span class="nav-number">1.2.</span> <span class="nav-text">Ⅰ. INTRODUCTION</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ⅱ-RELATED-WORK"><span class="nav-number">1.3.</span> <span class="nav-text">Ⅱ. RELATED WORK</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ⅲ-THE-PROPOSED-METHOD"><span class="nav-number">1.4.</span> <span class="nav-text">Ⅲ.  THE PROPOSED METHOD</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#A-Guided-Image-Filtering"><span class="nav-number">1.4.1.</span> <span class="nav-text">A. Guided Image Filtering</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#B-Guided-Convolution-Module"><span class="nav-number">1.4.2.</span> <span class="nav-text">B. Guided Convolution Module</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#IV-EXPERIMENTS"><span class="nav-number">1.5.</span> <span class="nav-text">IV. EXPERIMENTS</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#A-dataset"><span class="nav-number">1.5.1.</span> <span class="nav-text">A. dataset</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#B-Evaluation-Metrics"><span class="nav-number">1.5.2.</span> <span class="nav-text">B. Evaluation Metrics</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#C-Experiments-on-KITTI-Dataset"><span class="nav-number">1.5.3.</span> <span class="nav-text">C. Experiments on KITTI Dataset</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#D-Ablation-Studies"><span class="nav-number">1.5.4.</span> <span class="nav-text">D. Ablation Studies</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#E-Experiments-on-NYUv2-Dataset"><span class="nav-number">1.5.5.</span> <span class="nav-text">E. Experiments on NYUv2 Dataset</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#F-Generalization-Capability"><span class="nav-number">1.5.6.</span> <span class="nav-text">F. Generalization Capability</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#V-CONCLUSION"><span class="nav-number">1.6.</span> <span class="nav-text">V. CONCLUSION</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">kaixiang Liu</span>

  
</div>









        
<div class="busuanzi-count">
  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      访客数
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      人
    </span>
  

  
    <span class="site-pv">
      总访问量
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      次
    </span>
  
</div>








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  










  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  <script src="//unpkg.com/valine/dist/Valine.min.js"></script>
  
  <script type="text/javascript">
    var GUEST = ['nick','mail','link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item=>{
      return GUEST.indexOf(item)>-1;
    });
    new Valine({
        el: '#comments' ,
        verify: false,
        notify: false,
        appId: 'gQYYj7rbQ6uHgwmIHMOb9fn7-gzGzoHsz',
        appKey: 'GxriBt0fbPKm8K8m2Txay28O',
        placeholder: 'Just go go',
        avatar:'mm',
        guest_info:guest,
        pageSize:'10' || 10,
    });
  </script>



  





  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script>
  <script>AV.initialize("gQYYj7rbQ6uHgwmIHMOb9fn7-gzGzoHsz", "GxriBt0fbPKm8K8m2Txay28O");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  


  

  

</body>
</html>
