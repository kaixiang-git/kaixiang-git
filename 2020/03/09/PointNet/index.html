<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="论文阅读,计算机视觉,三维重建," />










<meta name="description" content="PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation在开始论文之前记录一下网页渲染引擎Nunjucks的坑，由于Hexo框架使用了Nunjucks引擎，而Nunjucks会将两个花括号“{{”当作标签信息，会与latex公式中的&quot;{{&quot;冲突，因此要在文章首尾加上“{% raw %}和{% endra">
<meta property="og:type" content="article">
<meta property="og:title" content="PointNet">
<meta property="og:url" content="http://yoursite.com/2020/03/09/PointNet/index.html">
<meta property="og:site_name" content="Blind Lover">
<meta property="og:description" content="PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation在开始论文之前记录一下网页渲染引擎Nunjucks的坑，由于Hexo框架使用了Nunjucks引擎，而Nunjucks会将两个花括号“{{”当作标签信息，会与latex公式中的&quot;{{&quot;冲突，因此要在文章首尾加上“{% raw %}和{% endra">
<meta property="og:image" content="http://yoursite.com/2020/03/09/PointNet/Applications-of-PointNet.png">
<meta property="og:image" content="http://yoursite.com/2020/03/09/PointNet/PointNet/Applications-of-PointNet.png">
<meta property="og:image" content="http://yoursite.com/2020/03/09/PointNet/Pipeline.png">
<meta property="og:image" content="http://yoursite.com/2020/03/09/PointNet/PointNet/Pipeline.png">
<meta property="og:image" content="http://yoursite.com/2020/03/09/PointNet/Tabel1.png">
<meta property="og:image" content="http://yoursite.com/2020/03/09/PointNet/PointNet/Tabel1.png">
<meta property="og:image" content="http://yoursite.com/2020/03/09/PointNet/Tabel2.png">
<meta property="og:image" content="http://yoursite.com/2020/03/09/PointNet/PointNet/Tabel2.png">
<meta property="og:image" content="http://yoursite.com/2020/03/09/PointNet/%E5%9B%BE3.png">
<meta property="og:image" content="http://yoursite.com/2020/03/09/PointNet/PointNet/%E5%9B%BE3.png">
<meta property="og:image" content="http://yoursite.com/2020/03/09/PointNet/Tabel3&Tabel4.png">
<meta property="og:image" content="http://yoursite.com/2020/03/09/PointNet/PointNet/Tabel3&Tabel4.png">
<meta property="og:image" content="http://yoursite.com/2020/03/09/PointNet/%E5%9B%BE5.png">
<meta property="og:image" content="http://yoursite.com/2020/03/09/PointNet/PointNet/%E5%9B%BE5.png">
<meta property="og:image" content="http://yoursite.com/2020/03/09/PointNet/Tabel5.png">
<meta property="og:image" content="http://yoursite.com/2020/03/09/PointNet/PointNet/Tabel5.png">
<meta property="og:image" content="http://yoursite.com/2020/03/09/PointNet/%E5%9B%BE6.png">
<meta property="og:image" content="http://yoursite.com/2020/03/09/PointNet/PointNet/%E5%9B%BE6.png">
<meta property="og:image" content="http://yoursite.com/2020/03/09/PointNet/%E5%9B%BE7.png">
<meta property="og:image" content="http://yoursite.com/2020/03/09/PointNet/PointNet/%E5%9B%BE7.png">
<meta property="og:image" content="http://yoursite.com/2020/03/09/PointNet/Tabel6.png">
<meta property="og:image" content="http://yoursite.com/2020/03/09/PointNet/PointNet/Tabel6.png">
<meta property="article:published_time" content="2020-03-09T10:00:49.000Z">
<meta property="article:modified_time" content="2020-03-09T11:02:46.126Z">
<meta property="article:author" content="kaixiang Liu">
<meta property="article:tag" content="论文阅读">
<meta property="article:tag" content="计算机视觉">
<meta property="article:tag" content="三维重建">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://yoursite.com/2020/03/09/PointNet/Applications-of-PointNet.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2020/03/09/PointNet/"/>





  <title>PointNet | Blind Lover</title>
  








<meta name="generator" content="Hexo 4.2.0"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Blind Lover</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/03/09/PointNet/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="kaixiang Liu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/zhenzhu.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Blind Lover">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">PointNet</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-03-09T18:00:49+08:00">
                2020-03-09
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%B2%BE%E8%AF%BB%E8%AE%BA%E6%96%87/" itemprop="url" rel="index">
                    <span itemprop="name">精读论文</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2020/03/09/PointNet/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2020/03/09/PointNet/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
             <span id="/2020/03/09/PointNet/" class="leancloud_visitors" data-flag-title="PointNet">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="PointNet-Deep-Learning-on-Point-Sets-for-3D-Classification-and-Segmentation"><a href="#PointNet-Deep-Learning-on-Point-Sets-for-3D-Classification-and-Segmentation" class="headerlink" title="PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation"></a>PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation</h1>在开始论文之前记录一下网页渲染引擎Nunjucks的坑，由于Hexo框架使用了Nunjucks引擎，而Nunjucks会将两个花括号“{{”当作标签信息，会与latex公式中的"{{"冲突，因此要在文章首尾加上“{% raw %}和{% endraw %}”。
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>点云是三维数据中重要的一种数据形式，但是由于其无序性，许多研究者将其转换为有序的体素结构或者从图像中收集信息。在这篇文章中我们设计了神经网络的一种新形式能够直接处理点云数据，而且能够很好的处理输入点云中的顺序无关性（指打乱点云的顺序后对结果不产生影响）。我们的PointNet为分类和分割任务提供了一个统一的网络结构。虽然简单，但是其高效且有用。它相比state of the art效果相当甚至更好。在理论方面，我们对于理解这个网络学习到的东西以及为什么这个网络很好的鲁棒性提供了分析。</p>
<a id="more"></a>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>在这篇文章中我们探讨了深度学习网络结构应用于3D数据的可行性例如点云和Mesh。典型的卷积操作需要输入数据有极强的位置关系来实现权重共享或者卷积核运算，例如2D图像的像素和3D体素。由于点云数据和Mesh网格不具有极强的位置关系，因此大多数研究者们将这些数据送入深度网络结构之前将其转换为3D体素或者从图像中获得深度信息。但是这些转换方式会使得生成的数据量非常巨大，但这并不是必要的。</p>
<p>基于这个原因，我们对于输入的3D数据使用简单的点云，将这个网络结构命名为PointNet。点云是简单统一的结构，避免了网格的组合不规则性和复杂性。因此更容易学习。但是点云只是一系列点的集合，其中元素的排列顺序的无关的，因此在网络结构中需要对称的结构。进一步还需要考虑刚性运动的其他不变性。</p>
<img src="/2020/03/09/PointNet/Applications-of-PointNet.png" class="">

<!-- <img src="./PointNet/Applications-of-PointNet.png" style="zoom:80%"  /> -->
<p>图1：PointNet的应用场景。我们提出了一个新颖的网络结构将原始点云作为输入不需要体素化或排序。他是一个统一的网络结构可以学习到局部和全局的特征，对于3D认知任务是一个简单的，高效的，有效果的方法。</p>
<p>我们的PointNet是一个统一的网络结构，输入为点云，输出为每一个输入的类别标签或者每个点的分割标签。我们的网络结构非常简单，在初始阶段每个点都是相同且独立的处理。在初始设置中每个点仅有其$x,y,z$三个坐标构成。其他的维度也可以加入，比如计算法向，局部或者全局的特征。</p>
<p>我们方法的关键之处在于使用了一个对称的函数，最大池化函数。网络有效的学习一组优化函数/标准，这些优化函数。标准能够在点云中选择感兴趣或者带有信息的点，并且对选择它们的原因进行了编码。最终全连接层将学习到的最佳值聚合到整个输入的全局描述器（即上文提到的分类任务）或者用来预测每一个点的标签（分割任务）。</p>
<p>我们的输入数据格式对于刚性变换和仿射变换同样有效，因为每个点是独立变换。因此我们添加了一个数据相关的空间变换网络，使得数据在进入PointNet之前规范化，同样也可以提高结果表现。</p>
<p>我们对所提出的方法进行了理论分析并且在实验中评估结果。我们发现我们的网络能够拟合任意的连续集合函数。更有趣的是，其能够证明我们的网络可以学习到用一些稀疏的关键点集合代表整个输入点云，可视化这些关键点集，其正好对应目标的骨架。理论分析部分解释了为什么PointNet对于输入中一些细微的扰动能够有很强的鲁棒性，比如噪声或者缺失的数据。</p>
<p>在一些分类，分割任务的benchmark数据集上，我们将PointNet与基于多视角或者体素的SOTA结果进行了对比。在统一的框架下PointNet不仅速度快而且显示出很强的效果，与SOTA的水平相当甚至更好。</p>
<p>我们的工作主要贡献如下：</p>
<ul>
<li>设计了一个新颖的网络结构，能够直接将无序的点云作为输入。</li>
<li>展示了这个网络对于分类，分割任务怎样训练</li>
<li>对于我们的方法在稳定性和效果上提供了理论分析</li>
<li>说明了网络中通过被选择的神经元计算出的3D特征并且对性能有直观的解释。</li>
</ul>
<p>对于无序几何使用神经网络是一个通用而且基础的问题，我们希望我们的idea能够被用于其他领域。</p>
<h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><p>Point Cloud Features：对于特定的任务大部分的点云特征都是手工设计的。点的特征通常表示点的某些统计属性，对于某种变换具有不变性。通常被分为内在的<a href="#2">[2]</a>,<a href="#24">[24]</a>,<a href="#3">[3]</a>，或者外在的<a href="#20">[20]</a>,<a href="#19">[19]</a>,<a href="#14">[14]</a>,<a href="#10">[10]</a>,<a href="#5">[5]</a>。他们也可以被分为局部或者全局的特征。对于特定的任务找到最优化的特征很重要。</p>
<p>Deep Learning On 3D Data：3D data有很多种表示方式，因此应用深度学习也产生了很多种方法。Volumetric CNNs<a href="#28">[28]</a>,<a href="#17">[17]</a>,<a href="#18">[18]</a>是将3D卷积神经网络用于体素结构的先锋。然而，体素结构受分辨率的限制，这些限制是由于数据稀疏性以及3D卷积操作的计算力而导致。FPNN<a href="#13">[13]</a>和Vote3D<a href="#26">[26]</a>对于数据稀疏的问题提出了特别的方法，但是他们仍然是在稀疏的体素上操作。对于他们，处理大量的点云数据是一个挑战。MultiViewCNNs<a href="#23">[23]</a>,<a href="#18">[18]</a>尝试将3D点云排序或者投影至2D图像然后应用2D卷积操作来分类。借助精心设计的图像CNN，这一系列方法在形状分类和检索任务上均取得了卓越的性能。但是，将它们扩展到场景理解或其他3D任务（例如点分类和3D形状补全）并非易事。Spectral CNNs：一些后来的工作<a href="#4">[4]</a>,<a href="#16">[16]</a> 在Mesh上应用频域卷积。但是，这些方法目前仅限于流形物体（例如有机物体）上，如何将其扩展到非等距形状（例如家具）尚不清楚。Feature-based DNNs：<a href="#6">[6]</a>,<a href="#8">[8]</a>首次通过提取传统形状特征将3D数据转换为矢量，然后使用全连接的网络对形状进行分类。我们认为它们受到提取特征的表示能力的限制。</p>
<p>Deep Learning on Unordered Sets：从数据结构的角度，点云是一组无序的向量。虽然大多数深度学习工作都集中在常规输入表示形式上，例如序列（语音和语言处理），图像和体积（视频或3D数据），但在深度学习上对点集的工作却很少。Oriol Vinyals等<a href="#25">[25]</a>的最新著作探讨了这个问题。他们使用具有注意力机制的read-process-write网络来处理无序输入集，并表明他们的网络具有对数字进行排序的能力。 但是，由于他们的工作集中在通用集合和NLP应用程序上，因此在集合中缺少几何的作用。</p>
<h2 id="Problem-Statement"><a href="#Problem-Statement" class="headerlink" title="Problem Statement"></a>Problem Statement</h2>我们设计了一个深度学习框架，该框架直接使用无序点集作为输入。点云有一个3D点的集合${\rm{\{ }}{{\rm{P}}_i}{\rm{|i = 1,}}...{\rm{,n\} }}$，每个点${{\rm{P}}_i}$由其$x,y,z$坐标和额外的特征通道如颜色，法向量等组成。简便起见，我们只用坐标来表示点。对于对象分类任务，可以直接从形状中采样输入点云，也可以从场景中预先分割输入点云。我们提出的网络对于$k$个类别会产生$k$个分数。对于语义分割，输入可以是用于部分区域分割的单个对象，也可以是3D场景中的子元素用于对象区域分割。我们的网络会产生$n \times m$个分数对应于$n$个点，每个点产生$m$个分数对应$m$个语义标签。

<h2 id="Deep-Learning-on-Point-Sets"><a href="#Deep-Learning-on-Point-Sets" class="headerlink" title="Deep Learning on Point Sets"></a>Deep Learning on Point Sets</h2><p>我们的网络结构(4.2节)是受到在${R^n}$上点集的性质的启发（4.1节）。</p>
<h3 id="4-1-Properties-of-Point-Sets-in-R-n"><a href="#4-1-Properties-of-Point-Sets-in-R-n" class="headerlink" title="4.1 Properties of Point Sets in ${R^n}$"></a>4.1 Properties of Point Sets in ${R^n}$</h3><p>我们的输入是欧几里得空间上点集的一个子集，其具有以下三条性质：</p>
<ul>
<li>Unordered.与图像中的像素阵列或体积网格中的体素阵列不同，点云是一组没有特定顺序的点。换句话说，输入为$N$个3D点集的网络必须对输入集的$N!$个排列的顺序都保持不变。</li>
<li>Interaction among points.这些点来自具有距离度量的空间，这意味着点不是孤立的，并且相邻点构成了有意义的子集。因此，模型需要能够从附近的点捕获局部结构，以及局部结构之间的组合相互作用。</li>
<li>Invariance under transformations.作为几何对象，学习到的模型应不变于某些变换。例如，旋转和平移点都不应改变全局点云类别或点的分割结果。</li>
</ul>
<h3 id="4-2-PointNet-Architecture"><a href="#4-2-PointNet-Architecture" class="headerlink" title="4.2 PointNet Architecture"></a>4.2 PointNet Architecture</h3><p>我们的完整网络架构如图2所示，其中分类网络和分割网络共享大部分结构。请阅读图2的标题以了解Pipeline。</p>
<img src="/2020/03/09/PointNet/Pipeline.png" class="">

<!-- <img src="./PointNet/Pipeline.png" style="zoom:80%"  /> -->
<p> PointNet Architecture：分类网络将n点作为输入，应用输入和特征转换，然后通过最大池化聚合点特征。输出是k个类别的分数。分割网络是分类网络的拓展。其将全局以及局部特征级联，输出每个点的分数。“mlp”代表多层感知器，括号中的数字是每一层的尺寸。 所有层的Batchnorm都为ReLu。分类网络的最后一层mlp使用了DropOut层。</p>
<p> 我们的网络有三个模块：最大池化层是一个对称函数，用于聚合来自所有点的信息，局部和全局信息的组合以及两个将输入点和特征点对齐的联合对齐网络。我们将会在下面的段落讨论网络这样设计以及选择的原因。</p>
<p> Symmetry Function for Unordered Input：为了使模型对输入的排列顺序具有不变行，存在三种策略：1）将输入按规范顺序排序。2）将输入作为序列，训练RNN，但通过各种排列来扩充训练数据。3）使用简单的对称函数聚合来自每个点的信息。在此，对称函数将$n$个向量作为输入，并输出一个与输入顺序不变的新向量。例如，$+$和$*$运算符是对称二进制函数。</p>
<p> 尽管排序听起来很简单，但实际上在高维空间中对于一般意义上的点摄动，不存在稳定的排序。可以很容易用反证法得到这个结论。如果存在这种排序策略，则它会定义高维空间和一维实线之间的双射图。不难发现，要求在点摄动的情况下有一个稳定的排序相当于要求这个特征图在维数减少时还能够保持空间邻近性，在这种情况下是不可能的。因此，排序无法完全解决顺序问题，而且随着顺序问题的持续存在，网络很难学习从输入到输出的一致映射。如实验所示（图5），我们发现直接在排序点集上应用MLP效果较差，尽管比直接处理未排序的输入要好。</p>
<p> 使用RNN的idea将点集视为顺序信号，并希望通过训练随机排列序列的RNN，RNN将对输入顺序不变。然而在<a href="#25">[25]</a>“OrderMatters”中作者证明了顺序很重要，不能被忽略。尽管RNN对长度较短（数十个）的序列的输入排序具有相对较好的鲁棒性，但很难扩展到数千个输入元素，这是点集的常见大小。根据经验，我们还证明，基于RNN的模型的性能不如我们提出的方法好（图5）。</p>
 我们的想法是通过对点集中的变换元素应用对称函数来近似定义点集上的一般函数：
$$f(\{ {x_1},...,{x_n}\} ) \approx g(h({x_1}),...,h({x_n}))\tag{1}$$
其中$f:{2^{{R^N}}} \to R,h:{R^N} \to {R^K}$还有$g:\underbrace {{R^K} \times  \cdots  \times {R^K}}_n \to R$是对称函数。

<p>从经验上讲，我们的基本模块非常简单：我们通过多层感知器网络估算h，通过单个变量函数和max pooling函数的组合估算g。通过实验发现这种方法效果很好。 通过收集$h$，我们可以学习一些$f’s$来捕获集合的不同属性。尽管我们的关键模块看起来很简单，但它具有一些有趣的属性（请参见5.3节），并且可以在一些不同的应用程序中实现出色的性能（请参见5.1节）。</p>
<p>Local and Global Information Aggregation ：上一节的输出形成向量$[{f_1}, \cdots ,{f_K}]$，它是输入集的全局特征。我们可以轻松地在形状全局特征上训练SVM或多层感知器分类器。但是，点分割需要结合局部和全局信息。我们可以通过简单而高效的方式来实现这一目标。</p>
<p>我们的方案如图2所示（Segmentation Network）。在计算了全局点云特征向量之后，我们通过将全局特征与每个点特征连接起来，将其反馈给每个点特征。然后我们基于已经结合的点特征，从中抽取出新的点特征，这时每个点的特征就具有了全局和局部信息。</p>
<p>通过这种修改，我们的网络可以预测每个点的性质，这些点同时具有局部几何信息以及全局信息。例如，我们可以准确地预测每点法线（补充图），从而验证网络能够汇总来自点本地邻域的信息。在实验环节中，我们还展示了我们的模型可以在形状分割和场景分割方面达到SOTA。</p>
<p>Joint Alignment Network：如果点云经历了某些几何变换（例如刚性变换），则该点云的语义标记必须是不变的。因此，我们希望通过我们的点集获得的学习表示对于这些变换是不变的。</p>
<p>一个自然的方案就是在特征提取之前将输入点集变换至标准空间（Canonical Space）。<a href="#9">[9]</a>介绍了空间变换器通过采样和插值来对齐2D图像的想法，这是通过在GPU上实现的特定定制层实现的。</p>
<p>我们的输入是点云的形式，因此这允许我们使用相比<a href="#9">[9]</a>更简单的方法来实现。在处理图像的情况下我们不需要引入新的层或者别名。我们通过微型网络（图2中的T-net）预测仿射变换矩阵，并将该变换直接应用于输入点的坐标。小型网络本身类似于大型网络，由针对点的独立的特征提取，最大池化和完全连接层等基本模块组成。 有关T-net的更多详细信息，请参见补充资料。</p>
<p>这种思想同样可以引申至特征空间的对齐。我们可以在特征点上插入另一个对齐网络，并预测一个特征转换矩阵以对齐来自不同输入点云的特征。然而，在特征空间上的转换矩阵相比空间转换矩阵有更高的维度，这极大的增加了优化的难度。因此，我们在softmax训练损失中添加了一个正规化项。 我们将特征变换矩阵约束为接近正交矩阵。<br>$${L_{reg}} = ||I - A{A^T}||_F^2 \tag{2}$$<br>其中$A$是由小型网络预测的特征对齐矩阵。正交变换将不会在输入中丢失信息，因此是需要的。 我们发现通过添加正则化项，优化变得更加稳定，并且我们的模型获得了更好的性能。</p>
<h3 id="4-3-Theoretical-Analysis"><a href="#4-3-Theoretical-Analysis" class="headerlink" title="4.3 Theoretical Analysis"></a>4.3 Theoretical Analysis</h3><p>Universal approximation：我们首先展示了我们的神经网络对连续集合函数的通用逼近能力。通过集合函数的连续性，直观地讲，对输入点集的微小摄动不应大大改变函数值，例如分类或分段得分。</p>
<p>令$\chi  = { S:S \subseteq {[0,1]^m}$and$|S| = n} ,f:\chi  \to R$是$\chi$上的连续集合函数，该集合函数遵循Hausdorff距离${d_H}( \cdot , \cdot )$，即$\forall \varepsilon  &gt; 0,\exists \delta  &gt; 0$,对于任意$S,S’ \in \chi$。如果${d_H}(S,S’) &lt; \delta$，那么$|f(S) - f(S’)| &lt; \varepsilon$。我们的理论证明如果在最大池化层有足够多的神经元那么我们的网络就可以任意的逼近$f$，即公式（1）中的$K$足够大。</p>

<p><strong>Theorem 1.</strong> 假定$f:\chi  \to R$是一个连续的集合函数，其遵循Hausdorff距离${d_H}( \cdot , \cdot )$，对于任意$\varepsilon&gt;0$，存在一个连续的函数$h$和一个对称函数$g({x_1}, \cdots ,{x_n}) = \gamma  \circ MAX$，这样对于任意$S \in \chi$有<br>$$|f(S) - \gamma (\mathop {MAX}\limits_{{x_i} \in S} { h({x_i})} )| &lt; \varepsilon \tag{3}$$<br>其中${x_1}, \cdots ,{x_n}$是$S$中任意顺序的所有元素，$\gamma$是一个连续函数，$MAX$是一个向量最大化的操作，输入为n个向量，返回一个新的向量，这个新的向量是输入n个向量的逐元素最大值。</p>
<p>这个理论的证明可以在补充材料中找到。关键思想是，在最坏的情况下，网络可以通过将空间划分为大小相等的体素来学习将点云转换为volumetric representation。然而，实际上，网络学到了一种更智能的策略来探测空间，正如我们将在点函数可视化中看到的那样。</p>
<p>Bottleneck dimension and stability：从理论上和实验上，我们发现网络的表现力受到最大池化层尺寸的强烈影响，即公式（1）中的$K$,这里我们提供一种分析，同样解释了我们模型的稳定性。</p>
<p>我们定义$u = \mathop {{\mathop{\rm MAX}\nolimits} }\limits_{{x_i} \in S} { h({x_i})}$是$f$的一个子网络，其将${[0,1]^m}$中的点集映射为K维的向量，下面的理论是说输入集中很小的摄动或者额外的噪点不会改变网络的输出。</p>
<p><strong>Theorem 2.</strong> 假定$u:\chi  \to {R^K}$这样$u = \mathop {MAX}\limits_{{x_i} \in S} { h({x_i})}$并且$f = \gamma  \circ u$,那么<br>$$(a)\quad \forall S,\exists {C_S},{N_S} \in \chi ,f(T) = f(S)if{C_S} \subseteq T \subseteq {N_S}$$<br>$$(b)\quad|{C_S}| \le K$$<br>我们解释了该定理的含义。公式（a）说如果${C_S}$中的点都被保留，那么$f(S)$不会被输入中的摄动所改变，同样也不会被噪点所改变。（b）说${C_S}$中仅包含了一定数量的点，取决于公式（1）中的$K$。换句话说，$f(S)$事实上取决于一个有限的子集${C_S} \subseteq S$或者少于或者等于$K$。我们因此将${C_S}$叫做$S$的临界点集，将$K$称为$f$的最大瓶颈。</p>
<p>结合$h$的连续性，这说明了我们的模型在摄动和额外噪声点方面的鲁棒性。类似于机器学习模型中的稀疏原理，获得了鲁棒性。 直观地，我们的网络学习通过稀疏的关键点来总结形状。 在实验部分，我们看到关键点形成了对象的骨架。</p>
{% raw %}
<h2 id="5-Experiment"><a href="#5-Experiment" class="headerlink" title="5. Experiment"></a>5. Experiment</h2><p>实验分为四个部分。首先我们展示了PointNets可以应用在多种3D识别任务上（5.1节）。第二，我们提供了具体的实验验证我们网络的设计（5.2节）。最后我们可视化了网络所学习到的东西（5.3节）并且分析了时间和空间复杂度。</p>
<p><strong>3D Object Classiﬁcation</strong>我们的网络能够学习到点云的全局特征来对其进行分类。我们在ModelNet40<a href="#28">[28]</a>形状分类的benchmark上评估了我们的模型。这里面有40类人工制造的12311个CAD模型，其中9843用于训练，2468用来测试。前面的工作都聚焦于体素或者多视角的表示，我们是首个直接在点云上应用的模型。</p>
<p>我们根据表面面积在Mesh表面统一采样了1024个点，并且将他们规则化。在训练过程中，我们通过沿上轴随机旋转对象并通过高斯噪声（均值为零且标准偏差为0.02）使每个点的位置摄动来增加点云数量。在表1中，我们将我们的模型与以前的工作进行了比较，并使用基于MLP的baseline对从点云中提取的传统特征（点密度，D2，形状轮廓等）进行了比较。我们的模型在基于3D输入（体积和点云）的方法中达到了SOTA。仅通过全连接层和最大池花层，我们的网络就可以在推理速度上取得领先优势，并且可以轻松地在CPU中并行化。但是我们的模型仍然与基于多视角的方法MVCNN<a href="#23">[23]</a>有微小的差距，我们认为应该是由于我们的方法丢失了渲染图像上精细的几何细节。</p>
<!-- <img src="/2020/03/09/PointNet/Tabel1.png" class=""> -->

<!-- <img src="./PointNet/Tabel1.png" style="zoom:80%"  /> -->

<p><strong>3D Object Part Segmentation：</strong> Part segmentation是细粒度3D识别任务的一个挑战。给定一个3D场景或者Mesh模型，目的是给每一个点或者表面分配不同的标签（比如椅子的腿，茶杯的把手）。</p>
<p>我们在ShapeNet数据集<a href="#29">[29]</a>上评估了我们的结果，ShapeNet包含16个类别的16881个形状，总共被标注为50个part。大部分的目标part种类是2-5个。Ground Truth标注在形状上采样的点云。</p>
<p>我们将part的分割当作每个点的分类来处理。评估指标是点的mIOU。对于每个具有$C$个part类别的形状$S$，计算其该形状的mIOU为：对于$C$个类别中的每一种类别计算真值与预测值之间的IoU。如果真值和预测值的并集是空集，那么将这部分的IoU记为1.然后将所有$C$个类别的IoU平均得到整个形状的mIoU。</p>
<p>在这一节，我们将模型对于分割任务的版本（图2中的修改版本 Segmentation Network）与两种传统的方法<a href="#27">[27]</a>，<a href="#29">[29]</a>以及我们自己的3D CNN baseline做对比，这两种方法都利用了逐点的几何特征和形状之间的联系。对于3D CNN的详细修改以及网络结构见补充材料。</p>
<p>在表2中，我们展示了每个类别的分数和平均IoU（%）的分数。我们观察到我们的方法在大多数类别的分割任务中打败了baseline的方法，平均IoU提高了2.3%。</p>
<img src="/2020/03/09/PointNet/Tabel2.png" class="">

<!-- <img src="./PointNet/Tabel2.png" /> -->
<p>我们还通过模拟Kinect扫描进行实验，以测试ShapeNet零件数据集中每个cad模型的这些方法的鲁棒性，我们使用Blensor Kinect Simulator<a href="#7">[7]</a>从六个随机视角生成不完整的点云。我们使用相同的网络体系结构和训练设置对pointNet进行完整形状和部分场景的训练。结果显示我们的平均IoU仅仅低了5.3%。在图3中，我们展示了完整数据和部分数据的定性结果。可以看出虽然部分数据很有挑战性，但是我们的预测同样合理。</p>
<img src="/2020/03/09/PointNet/%E5%9B%BE3.png" class="">

<!-- <img src="./PointNet/图3.png" /> -->

<p><strong>Semantic Segmentation in Scenes：</strong> 我们的part分割网络可以轻松扩展到语义场景分割，其中点标签成为语义对象类，而不是对象part标签。</p>
<p>我们在斯坦福3D语义解析数据集<a href="#1">[1]</a>上进行实验。该数据集包含 Matterport扫描仪在6个区域的271个房间的3D场景。场景中的每个点被标注为13个类别标签的其中之一（椅子，桌子，地板，墙等等）。</p>
<p>为了准备训练数据，我们首先将点按room分割，然后将每个room的点采样为1米×1米的block。使用PointNet的分割版本来预测block中每个点的类别。每个点由一个9维的向量表示：XYZ坐标，RGB还有相对房间的归一化位置。在训练时，我们在每个block中随机采样4096个点。我们遵循与<a href="#1">[1]</a>相同的协议，使用k折策略进行训练和测试。</p>
<p>我们将我们的方法与使用手工点特征的baseline对比。该baseline抽取了同样的9维局部特征和另外三个特征：局部点密度，局部曲率和法线。我们使用标准MLP作为分类器。结果在表3中，我们的PointNet方法明显优于baseline方法。在图4中，我们展示了分割的定性结果。我们的网络能够输出平滑的预测，并且对于缺失点和遮挡具有鲁棒性。</p>
<p>基于网络输出的语义分割，我们进一步构建了一个使用连接组件进行目标提议的3D目标检测系统（有关详细信息，请参见补充资料）。我们在表4中与先前的SOTA方法进行了比较。先前的方法基于滑动形状方法（使用CRF后处理），并且对体素网格中的局部几何特征和全局room上下文特征进行了SVMS训练。 在所报告的家具类别中，我们的方法大大优于该方法。</p>
<img src="/2020/03/09/PointNet/Tabel3&Tabel4.png" class="">
<!-- <img src="./PointNet/Tabel3&Tabel4.png" /> -->


<h3 id="5-2-Architecture-Design-Analysis"><a href="#5-2-Architecture-Design-Analysis" class="headerlink" title="5.2 Architecture Design Analysis"></a>5.2 Architecture Design Analysis</h3><p>在本节中，我们通过控制实验来验证我们的设计选择。 我们还显示了网络超参数的影响。</p>
<p><strong>Comparison with Alternative Order-invariant Methods：</strong>4.2节中提到，有至少3中方法可以对输入点集中的无序性保持鲁棒性。我们使用ModelNet40数据集作为对比实验的数据集。图5中我们比较的baseline包括在大小为$n \times 3$的排序和未排序的点集上应用多层感知机，将点集当作序列的RNN模型，基于对称函数的模型。我们实验所用的对称操作包括最大池化操作，平均池化操作和基于权重相加的注意力机制。注意力机制与<a href="#25">[25]</a>中相似，其中从每个点特征预测标量得分，然后通过计算softmax对得分进行标准化。然后根据归一化的分数和点特征计算加权和，如图5所示。max pooling操作实现了最佳性能，这验证了我们的选择。</p>
<img src="/2020/03/09/PointNet/%E5%9B%BE5.png" class="">
<!-- <img src="./PointNet/图5.png" /> -->

<p><strong>Effectiveness of Input and Feature Transformations：</strong> 在表5中，我们演示了输入和特征转换（用于对齐）的效果。有趣的是，最基本的体系结构已经实现了相当合理的结果。 使用输入转换可将性能提高0.8％。正则化损失对于高维变换的工作是必不可少的，通过结合两个变换和正则化项，我们可以获得最佳性能。</p>
<img src="/2020/03/09/PointNet/Tabel5.png" class="">
<!-- <img src="./PointNet/Tabel5.png" /> -->

<p><strong>Robustness Test：</strong> 我们展示了我们的PointNet，尽管简单有效，但对各种输入摄动具有鲁棒性。我们使用与图5的max pooling network中相同的体系结构。输入点归一化为标准空间。结果如图6所示。</p>
<p>至于缺失点，相对于最远和随机的输入采样，当缺失50％的点时，准确率仅下降2. 4％和3. 8％。我们评估了两种模型：一种是只具有(X,Y,Z)坐标的点，另一种是(X,Y,Z)坐标加点密度。当有20%是异常点时，网络还能达到80%的准确率。图6显示了网络对于点摄动的鲁棒性。</p>
<img src="/2020/03/09/PointNet/%E5%9B%BE6.png" class="">
<!-- <img src="./PointNet/图6.png" /> -->

<h3 id="5-3-Visualizing-PointNet"><a href="#5-3-Visualizing-PointNet" class="headerlink" title="5.3 Visualizing PointNet"></a>5.3 Visualizing PointNet</h3><p>在图7中，我们可视化了一些样本形状$S$的临界点集${C_S}$和上限点集${N_S}$（如Theorem 2.中所述）。 两个形状之间的点集将给出完全相同的全局形状特征$f(S)$。</p>
<p>我们可以从图7中看到，临界点集${C_S}$，那些组成最大池化特征的点构成了整个形状的骨架。上限点集${N_S}$表示最大可能的点云，该点云具有与输入点云$S$相同的全局特征。${C_S}$和${N_S}$反映了PointNet的鲁棒性，这意味着丢失一些非关键点根本不会改变全局形状特征$f(S)$。</p>
<p>${N_S}$是通过以边长为2的立方体前向传播网络中的所有点来构造的。并且选择不大于全局形状描述符的点集$p$，作用于该点集的函数值等于$({h_1}(p),{h_2}(p), \cdots ,{h_K}(p))$。</p>
<img src="/2020/03/09/PointNet/%E5%9B%BE7.png" class="">
<!-- <img src="./PointNet/图7.png" /> -->

<h3 id="5-4-Time-and-Space-Complexity-Analysis"><a href="#5-4-Time-and-Space-Complexity-Analysis" class="headerlink" title="5.4 Time and Space Complexity Analysis"></a>5.4 Time and Space Complexity Analysis</h3><p>表6总结了我们PointNet分类网络的空间（网络中的参数数量）和时间（浮点运算/样本复杂度）复杂度。我们还将PointNet与之前的工作中一组具有代表性的基于体积和多视图的体系结构进行比较。</p>
<p>尽管MVCNN<a href="#23">[23]</a>和Subvolume<a href="#18">[18]</a>（3D CNN）有更高的准确性，但是PointNet在计算开销上更高效（在FLOPs/sample上相比分别有141倍和8倍的差距）。除此之外就网络中的参数量来说（参数少了17倍），PointNet在空间复杂度上相比MVCNN也更高效。而且PointNet更加具有可扩展性，他的时间以及空间复杂度都是$O(N)$，随着输入点云的数量而线性增长。然而，由于卷积运算控制着计算时间，因此多视图方法的时间复杂度平方正比于图像分辨率，而基于体积卷积的方法则随着体积的大小而立方增长。</p>
<p>根据实验，在TensorFlow上使用1080X GPU，PointNet每秒可以处理超过一百万个点以进行点云分类（大约1K个对象/秒）或语义分割（大约2个room/秒），在时间效率上显示出巨大的潜力。</p>
<img src="/2020/03/09/PointNet/Tabel6.png" class="">
<!-- <img src="./PointNet/Tabel6.png" /> -->



<h2 id="6-Conclusion"><a href="#6-Conclusion" class="headerlink" title="6. Conclusion"></a>6. Conclusion</h2><p>在这项工作中，我们提出了一种直接输入点云的新颖的深度神经网络PointNet。 我们的网络为许多3D识别任务提供了统一的方法，包括目标分类，目标part分割和语义分割，同时与baseline相比达到了SOTA。为了理解我们的网络，我们还提供理论分析和可视化。</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p><span id="1"> [1]: Armeni, I., Sener, O., Zamir, A. R., Jiang, H., Brilakis, I., Fischer, M., &amp; Savarese, S. (2016). 3d semantic parsing of large-scale indoor spaces. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 1534-1543).</p>
<p><span id="2"> [2]: Aubry, M., Schlickewei, U., &amp; Cremers, D. (2011, November). The wave kernel signature: A quantum mechanical approach to shape analysis. In 2011 IEEE international conference on computer vision workshops (ICCV workshops) (pp. 1626-1633). IEEE.</p>
<p><span id="3"> [3]: Bronstein, M. M., &amp; Kokkinos, I. (2010, June). Scale-invariant heat kernel signatures for non-rigid shape recognition. In 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (pp. 1704-1711). IEEE.</p>
<p><span id="4"> [4]: Bruna, J., Zaremba, W., Szlam, A., &amp; LeCun, Y. (2013). Spectral networks and locally connected networks on graphs. arXiv preprint arXiv:1312.6203.</p>
<p><span id="5"> [5]: Chen, D. Y., Tian, X. P., Shen, Y. T., &amp; Ouhyoung, M. (2003, September). On visual similarity based 3D model retrieval. In Computer graphics forum (Vol. 22, No. 3, pp. 223-232). Oxford, UK: Blackwell Publishing, Inc.</p>
<p><span id="6"> [6]: Fang, Y., Xie, J., Dai, G., Wang, M., Zhu, F., Xu, T., &amp; Wong, E. (2015). 3d deep shape descriptor. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 2319-2328).</p>
<p><span id="7"> [7]: Gschwandtner, M., Kwitt, R., Uhl, A., &amp; Pree, W. (2011). BlenSor: blender sensor simulation toolbox advances in visual computing. volume 6939 of lecture notes in computer science, chapter 20. Springer Berlin/Heidelberg, Berlin, Heidelberg, 4, 199-208.</p>
<p><span id="8"> [8]: Guo, K., Zou, D., &amp; Chen, X. (2015). 3d mesh labeling via deep convolutional neural networks. ACM Transactions on Graphics (TOG), 35(1), 1-12.</p>
<p><span id="9"> [9]: Jaderberg, M., Simonyan, K., &amp; Zisserman, A. (2015). Spatial transformer networks. In Advances in neural information processing systems (pp. 2017-2025).</p>
<p><span id="10"> [10]: Johnson, A. E., &amp; Hebert, M. (1999). Using spin images for efficient object recognition in cluttered 3D scenes. IEEE Transactions on pattern analysis and machine intelligence, 21(5), 433-449.</p>
<p><span id="11"> [11]: Kazhdan, M., Funkhouser, T., &amp; Rusinkiewicz, S. (2003, June). Rotation invariant spherical harmonic representation of 3 d shape descriptors. In Symposium on geometry processing (Vol. 6, pp. 156-164).</p>
<p><span id="12"> [12]: LeCun, Y., Bottou, L., Bengio, Y., &amp; Haffner, P. (1998). Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11), 2278-2324.</p>
<p><span id="13"> [13]: Li, Y., Pirk, S., Su, H., Qi, C. R., &amp; Guibas, L. J. (2016). Fpnn: Field probing neural networks for 3d data. In Advances in Neural Information Processing Systems (pp. 307-315).</p>
<p><span id="14"> [14]:  Ling, H., &amp; Jacobs, D. W. (2007). Shape classification using the inner-distance. IEEE transactions on pattern analysis and machine intelligence, 29(2), 286-299.</p>
<p><span id="15"> [15]:  Maaten, L. V. D., &amp; Hinton, G. (2008). Visualizing data using t-SNE. Journal of machine learning research, 9(Nov), 2579-2605.</p>
<p><span id="16"> [16]:  Masci, J., Boscaini, D., Bronstein, M., &amp; Vandergheynst, P. (2015). Geodesic convolutional neural networks on riemannian manifolds. In Proceedings of the IEEE international conference on computer vision workshops (pp. 37-45).</p>
<p><span id="17"> [17]: Maturana, D., &amp; Scherer, S. (2015, September). Voxnet: A 3d convolutional neural network for real-time object recognition. In 2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) (pp. 922-928). IEEE.</p>
<p><span id="18"> [18]: Qi, C. R., Su, H., Nießner, M., Dai, A., Yan, M., &amp; Guibas, L. J. (2016). Volumetric and multi-view cnns for object classification on 3d data. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 5648-5656).</p>
<p><span id="19"> [19]: Rusu, R. B., Blodow, N., &amp; Beetz, M. (2009, May). Fast point feature histograms (FPFH) for 3D registration. In 2009 IEEE international conference on robotics and automation (pp. 3212-3217). IEEE.</p>
<p><span id="20"> [20]: Rusu, R. B., Blodow, N., Marton, Z. C., &amp; Beetz, M. (2008, September). Aligning point cloud views using persistent feature histograms. In 2008 IEEE/RSJ International Conference on Intelligent Robots and Systems (pp. 3384-3391). IEEE.</p>
<p><span id="23"> [23]: Su, H., Maji, S., Kalogerakis, E., &amp; Learned-Miller, E. (2015). Multi-view convolutional neural networks for 3d shape recognition. In Proceedings of the IEEE international conference on computer vision (pp. 945-953).</p>
<p><span id="24"> [24]: Sun, J., Ovsjanikov, M., &amp; Guibas, L. (2009, July). A concise and provably informative multi‐scale signature based on heat diffusion. In Computer graphics forum (Vol. 28, No. 5, pp. 1383-1392). Oxford, UK: Blackwell Publishing Ltd.</p>
<p><span id="25"> [25]: Vinyals, O., Bengio, S., &amp; Kudlur, M. (2015). Order matters: Sequence to sequence for sets. arXiv preprint arXiv:1511.06391.</p>
<p><span id="26"> [26]: Wang, D. Z., &amp; Posner, I. (2015, July). Voting for Voting in Online Point Cloud Object Detection. In Robotics: Science and Systems (Vol. 1, No. 3, pp. 10-15607).</p>
<p><span id="27"> [27]: Wu, Z., Shou, R., Wang, Y., &amp; Liu, X. (2014). Interactive shape co-segmentation via label propagation. Computers &amp; Graphics, 38, 248-254.</p>
<p><span id="28"> [28]: Wu, Z., Song, S., Khosla, A., Yu, F., Zhang, L., Tang, X., &amp; Xiao, J. (2015). 3d shapenets: A deep representation for volumetric shapes. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 1912-1920).</p>
<p><span id="29"> [29]: Yi, L., Kim, V. G., Ceylan, D., Shen, I. C., Yan, M., Su, H., … &amp; Guibas, L. (2016). A scalable active framework for region annotation in 3d shape collections. ACM Transactions on Graphics (TOG), 35(6), 1-12.</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" rel="tag"># 论文阅读</a>
          
            <a href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/" rel="tag"># 计算机视觉</a>
          
            <a href="/tags/%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA/" rel="tag"># 三维重建</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2020/03/03/A-survey-on-Deep-Geometry-Learning-From-a-Representation-Perspective/" rel="next" title="A-survey-on-Deep-Geometry-Learning-From-a-Representation-Perspective">
                <i class="fa fa-chevron-left"></i> A-survey-on-Deep-Geometry-Learning-From-a-Representation-Perspective
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
    </div>
  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/zhenzhu.png"
                alt="kaixiang Liu" />
            
              <p class="site-author-name" itemprop="name">kaixiang Liu</p>
              <p class="site-description motion-element" itemprop="description">知心大姐姐的臭弟弟的blog</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives">
              
                  <span class="site-state-item-count">3</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">2</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">5</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/kaixiang-git" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://weibo.com/BlindLover0403" target="_blank" title="微博">
                      
                        <i class="fa fa-fw fa-globe"></i>微博</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://www.zhihu.com/people/liu-kai-40-21/activities" target="_blank" title="知乎">
                      
                        <i class="fa fa-fw fa-globe"></i>知乎</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#PointNet-Deep-Learning-on-Point-Sets-for-3D-Classification-and-Segmentation"><span class="nav-number">1.</span> <span class="nav-text">PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract"><span class="nav-number">1.1.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Introduction"><span class="nav-number">1.2.</span> <span class="nav-text">Introduction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Related-Work"><span class="nav-number">1.3.</span> <span class="nav-text">Related Work</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Problem-Statement"><span class="nav-number">1.4.</span> <span class="nav-text">Problem Statement</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Deep-Learning-on-Point-Sets"><span class="nav-number">1.5.</span> <span class="nav-text">Deep Learning on Point Sets</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-Properties-of-Point-Sets-in-R-n"><span class="nav-number">1.5.1.</span> <span class="nav-text">4.1 Properties of Point Sets in ${R^n}$</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-PointNet-Architecture"><span class="nav-number">1.5.2.</span> <span class="nav-text">4.2 PointNet Architecture</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-3-Theoretical-Analysis"><span class="nav-number">1.5.3.</span> <span class="nav-text">4.3 Theoretical Analysis</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-Experiment"><span class="nav-number">1.6.</span> <span class="nav-text">5. Experiment</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#5-2-Architecture-Design-Analysis"><span class="nav-number">1.6.1.</span> <span class="nav-text">5.2 Architecture Design Analysis</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-3-Visualizing-PointNet"><span class="nav-number">1.6.2.</span> <span class="nav-text">5.3 Visualizing PointNet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-4-Time-and-Space-Complexity-Analysis"><span class="nav-number">1.6.3.</span> <span class="nav-text">5.4 Time and Space Complexity Analysis</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-Conclusion"><span class="nav-number">1.7.</span> <span class="nav-text">6. Conclusion</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References"><span class="nav-number">1.8.</span> <span class="nav-text">References</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">kaixiang Liu</span>

  
</div>









        
<div class="busuanzi-count">
  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      访客数
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      人
    </span>
  

  
    <span class="site-pv">
      总访问量
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      次
    </span>
  
</div>








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  










  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  <script src="//unpkg.com/valine/dist/Valine.min.js"></script>
  
  <script type="text/javascript">
    var GUEST = ['nick','mail','link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item=>{
      return GUEST.indexOf(item)>-1;
    });
    new Valine({
        el: '#comments' ,
        verify: false,
        notify: false,
        appId: 'gQYYj7rbQ6uHgwmIHMOb9fn7-gzGzoHsz',
        appKey: 'GxriBt0fbPKm8K8m2Txay28O',
        placeholder: 'Just go go',
        avatar:'mm',
        guest_info:guest,
        pageSize:'10' || 10,
    });
  </script>



  





  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script>
  <script>AV.initialize("gQYYj7rbQ6uHgwmIHMOb9fn7-gzGzoHsz", "GxriBt0fbPKm8K8m2Txay28O");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  


  

  

</body>
</html>
